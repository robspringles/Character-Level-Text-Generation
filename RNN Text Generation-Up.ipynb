{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Text Generation\n",
    "\n",
    "X = inputs are a sequence of characters (eg. Shakespeare plays)\n",
    "\n",
    "Y = targets are the next char in the sequence\n",
    "\n",
    "eg. X = \"Hello World\" Y = \"ello World.\"\n",
    "\n",
    "Characters are one-hot encoded into an ASCII encoding for embedding before input into the model\n",
    "\n",
    "Character set is ASCII, using python's built in methods, -1 is used for unknown chars\n",
    "\n",
    "Sequence = string of characters of length sequence length for training/ testing (generated)\n",
    "\n",
    "Batch size = number of sequences per training batch\n",
    "\n",
    "Bibliography:\n",
    "\n",
    "\"TensorFlow without a PhD\" by Martin Gorner https://goo.gl/jrd7AR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf # Build and run the RNN model\n",
    "import numpy as np # Numerical ops and lin alg.\n",
    "import pandas as pd # File i/o\n",
    "import glob\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Magic Numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBED_SIZE = 127 # Number of distinct chars we are using to write/ read from text\n",
    "# 127 is ascii chars\n",
    "# 350 includes many accented vowels\n",
    "# More rare chars included makes for a more memory-expensive embedding\n",
    "CELL_SIZE = 512 # Number of units in our GRU cell\n",
    "N_LAYERS = 3 # Number of stacked GRU cells in our deep-cell (multi-layer)\n",
    "BATCH_SIZE = 150\n",
    "SEQ_LEN = 30\n",
    "INPUT_PATH = \"./messages/*.txt\" # Path to list of input texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "N_TO_GENERATE = 1000 # How many novel characters to generate?\n",
    "# Text generation init char\n",
    "INITIAL_CHAR = \"O\"\n",
    "#INITIAL_CHAR = input(\"Text generation start char: \") # Input for initial char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def timestamp():\n",
    "    return str(round(time.time()))\n",
    "def datestamp():\n",
    "    return time.strftime(\"%c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert from string of chars to list of ints or vice-versa, iff the code is within our embedding\n",
    "def char_to_int(x):\n",
    "    return [ord(char) for char in x if ord(char) <= EMBED_SIZE]\n",
    "\n",
    "def int_to_char(x):\n",
    "    return \"\".join([chr(int_) for int_ in x if int_ <= EMBED_SIZE])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# path [file path to directory of input data files]\n",
    "# pct [lower bound pct to use as validation]\n",
    "def read_file(path, pct=0.0):\n",
    "    data = {}\n",
    "    # What files do we have?\n",
    "    path_list = glob.glob(path, recursive=True)\n",
    "    if path_list == None or len(path_list) < 1: \n",
    "        assert \"Error None or empty file list, files {} in path {}\".format(path_list, path)\n",
    "    # Open the files\n",
    "    for file in path_list:\n",
    "        print(\"Loading file {}\".format(file))\n",
    "        with open(file, 'r') as f:\n",
    "            text = char_to_int(f.read())\n",
    "        data[file] = text\n",
    "    # Split into train and validation\n",
    "    total = sum([len(i) for _, i in data.items()]) # How many chars we have\n",
    "    if total < 1:\n",
    "        assert \"No data found in files\"\n",
    "    frac = int(total * pct) # How much to use as validation\n",
    "    # Pick the two or three smallest data files (whichever closest to frac)\n",
    "    short = sorted(data, key=lambda k: len(data[k]), reverse=False)\n",
    "    # Add files to validation until we exceed frac\n",
    "    vl = 0 # record length of validation text to use\n",
    "    index = 0 # record highest index into list of data to use\n",
    "    for i in short:\n",
    "        index += 1\n",
    "        vl += len(data[i])\n",
    "        if vl > frac:\n",
    "            break\n",
    "    print(\"\\nTotal data {} chars, using {} as validation ({:.2f}%)\\n\".format(total, vl, (vl * 100)/ (total - vl)))\n",
    "    validation = []\n",
    "    [validation.extend(data[i]) for i in short[:index]]\n",
    "    train = []\n",
    "    [train.extend(data[i]) for i in short[index:]]\n",
    "    return train, validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Args: \n",
    "# path [path to input text files]\n",
    "# bs [batch size]\n",
    "# sl [sequence length (within a batch)]\n",
    "# n_epochs [number of epochs to run through]\n",
    "# Will yield batches of pre-processed data ready for RNN input n_epochs times over all data\n",
    "def get_batch(data, bs, sl, n_epochs=30):\n",
    "    data = np.array(data)\n",
    "    dl = data.shape[0] # We use data length - 1 because we also use sequence shifted by one as target\n",
    "    n_batches = (dl - 1) // (bs * sl)\n",
    "    if n_batches < 1: assert \"Data length {} not enough for batch size {}\".format(dl, bs)\n",
    "    dl = n_batches * bs * sl # Use the rounded data length\n",
    "    x_data = np.reshape(data[0:dl], [bs, n_batches * sl])\n",
    "    y_data = np.reshape(data[1:dl + 1], [bs, n_batches * sl])\n",
    "    count = 0 # For progress\n",
    "    for epoch in range(n_epochs):\n",
    "        for batch in range(n_batches):\n",
    "            x = x_data[:, batch * sl:(batch + 1) * sl]\n",
    "            y = y_data[:, batch * sl:(batch + 1) * sl]\n",
    "            # Continue text from epoch to epoch by rolling back to start (don't reset RNN state each epoch)\n",
    "            x = np.roll(x, -epoch, axis=0)\n",
    "            y = np.roll(y, -epoch, axis=0)\n",
    "            count += 1\n",
    "            yield x, y, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample(p, top=EMBED_SIZE):\n",
    "    # Pick a random choice of p within the top n probabilities, weighted by their probability within p\n",
    "    p = np.squeeze(p)\n",
    "    p[np.argsort(p)[:-top]] = 0\n",
    "    p = p / np.sum(p)\n",
    "    return np.random.choice(EMBED_SIZE, 1, p=p)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA, VALIDATION_DATA = read_file(INPUT_PATH) \n",
    "print(\"\\nTRAIN\\n\\n\")\n",
    "print(int_to_char(TRAIN_DATA[:100]))\n",
    "print(\"\\nVALIDATION\\n\\n\")\n",
    "print(int_to_char(VALIDATION_DATA[:100]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "X = tf.placeholder(tf.uint8, [None, None]) # Input array\n",
    "X_one_hot = tf.one_hot(X, EMBED_SIZE, 1.0, 0.0) # One hot encode inputs\n",
    "Y_ = tf.placeholder(tf.uint8, [None, None]) # Targets, the actual next char\n",
    "Y_one_hot = tf.reshape(tf.one_hot(Y_, EMBED_SIZE, 1.0, 0.0), [-1, EMBED_SIZE]) # One hot targets\n",
    "Hin = tf.placeholder(tf.float32, [None, CELL_SIZE * N_LAYERS])\n",
    "batch_size = tf.placeholder(tf.int32)\n",
    "dropout_keep_prob = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# RNN Cells with dropout\n",
    "cells = [tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.GRUCell(CELL_SIZE), \n",
    "                                       input_keep_prob=dropout_keep_prob) for _ in range(N_LAYERS)]\n",
    "deep_cell = tf.contrib.rnn.DropoutWrapper(tf.contrib.rnn.MultiRNNCell(cells, state_is_tuple=False), \n",
    "                                          output_keep_prob=dropout_keep_prob)\n",
    "Yr, H = tf.nn.dynamic_rnn(deep_cell, X_one_hot, initial_state=Hin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Softmax readout layer\n",
    "Yf = tf.reshape(Yr, [-1, CELL_SIZE])\n",
    "logits = tf.contrib.layers.linear(Yf, EMBED_SIZE) # (WX + B)\n",
    "Y = tf.nn.softmax(logits) # Our predicted probabilities for next char\n",
    "prediction = tf.argmax(Y, 1) # Our predicted likely next char\n",
    "prediction = tf.reshape(prediction, [batch_size, -1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y_one_hot)\n",
    "loss = tf.reshape(loss, [batch_size, -1])\n",
    "seq_loss = tf.reduce_mean(loss, 1)\n",
    "batch_loss = tf.reduce_mean(seq_loss)\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(Y_, tf.cast(prediction, tf.uint8)), tf.float32))\n",
    "lsum = tf.summary.scalar(\"batch-loss\", batch_loss)\n",
    "asum = tf.summary.scalar(\"accuracy\", accuracy)\n",
    "summary = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Training step\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Run the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Summary writer for TensorBoard progress and checkpoint saving\n",
    "summary_writer = tf.summary.FileWriter(\"log/\" + timestamp(), sess.graph)\n",
    "saver = tf.train.Saver(max_to_keep=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run our training loop\n",
    "inH = np.zeros([BATCH_SIZE, CELL_SIZE * N_LAYERS]) # Initial in state\n",
    "for epoch in range(20):\n",
    "    # Get our x and y data batch and the count of number of batches\n",
    "    for x, y_, count in get_batch(TRAIN_DATA, BATCH_SIZE, SEQ_LEN, n_epochs=30):\n",
    "        # Training loop\n",
    "        _, outH = sess.run([train_step, H], feed_dict={\n",
    "            X: x, Y_: y_, Hin: inH, dropout_keep_prob: 0.8, batch_size: BATCH_SIZE\n",
    "        })\n",
    "        # Print out full training comparison\n",
    "        if count % 25 == 0:\n",
    "            pred, bl, sl, acc, sy = sess.run([prediction, batch_loss, seq_loss, accuracy, summary],feed_dict={\n",
    "                X: x, Y_: y_, Hin: inH, dropout_keep_prob: 1.0, batch_size: BATCH_SIZE\n",
    "            })\n",
    "            print(\"=\"*110)\n",
    "            for i in range(10):\n",
    "                print(\"== TRAINING TEXT ==\\n\")\n",
    "                print(int_to_char(x[i]))\n",
    "                print(\"-\"*110)\n",
    "                print(\"== PREDICTED TEXT ==\\n\")\n",
    "                print(int_to_char(pred[i]))\n",
    "                print(\"\\nLoss: {:.3f}\".format(sl[i]))\n",
    "                print(\"-\"*110)\n",
    "            print(\"{} batch [{}], batch-loss [{:.4f}], accurcay [{:.4f}]\".format(datestamp(), count, bl, acc))\n",
    "            print(\"=\"*110)\n",
    "        # Print out loss and accuracy, and add a TensorBoard summary (elif bc some overlap with full training comparison)\n",
    "        elif count % 5 == 0:\n",
    "            bl, acc, sy = sess.run([batch_loss, accuracy, summary],feed_dict={\n",
    "                X: x, Y_: y_, Hin: inH, dropout_keep_prob: 1.0, batch_size: BATCH_SIZE\n",
    "            })\n",
    "            print(\"{} batch [{}], batch-loss [{:.4f}], accurcay [{:.4f}]\".format(datestamp(), count, bl, acc))\n",
    "            summary_writer.add_summary(sy, count * BATCH_SIZE * SEQ_LEN)  \n",
    "        # Save checkpoint\n",
    "        if count % 150 == 0:\n",
    "            to = saver.save(sess, 'checkpoints/rnn_train-' + timestamp(), global_step=count)\n",
    "            print(\"Saved to {}\".format(to))\n",
    "        # Print generated text\n",
    "        if count % 50 == 0:\n",
    "            print()\n",
    "            print(\"=\"*110)\n",
    "            print(\"== GENERATED TEXT ==\\n\")\n",
    "            print(INITIAL_CHAR, end=\"\")\n",
    "            ry = np.array([char_to_int(INITIAL_CHAR)])\n",
    "            rh = np.zeros([1, CELL_SIZE * N_LAYERS])\n",
    "            for i in range(N_TO_GENERATE):\n",
    "                ryo, rh = sess.run([Y, H], feed_dict={\n",
    "                    X: ry, dropout_keep_prob: 1.0, Hin: rh, batch_size: 1\n",
    "                })\n",
    "                rc = sample(ryo, top=2) # Sample from the top probabilites\n",
    "                print(int_to_char([rc.tolist()]), end=\"\")\n",
    "                ry = np.array([[rc]])\n",
    "            print()\n",
    "            print(\"=\"*110)\n",
    "        # Loop the state back in\n",
    "        inH = outH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save final model\n",
    "to = saver.save(sess, 'checkpoints/final-model-' + timestamp(), global_step=count)\n",
    "print(\"Saved to {}\".format(to))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "\n",
    "TO-DO: \n",
    "\n",
    "Name variables in \"Define Graph\" section, so can handle them below when restore graph as right now I can't seem to access them when restoring graph!\n",
    "\n",
    "Use string instead of char init: needs to iterate through string (for loop) printing the string and feeding through to train the hidden state on the input string, before the while loop feeding back chars into itself to generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "sess = tf.InteractiveSession()\n",
    "restorer = tf.train.import_meta_graph('checkpoints/final-model-1499204970-5407.meta')\n",
    "restorer.restore(sess, \"checkpoints/final-model-1499204970-5407\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_char = input(\"Please enter initial character: \") # Make it take a series of chars\n",
    "print(\"=\"*110)\n",
    "print(\"== GENERATED TEXT ==\\n\")\n",
    "print(init_char, end=\"\")\n",
    "ry = np.array([char_to_int(init_char)])\n",
    "rh = np.zeros([1, CELL_SIZE * N_LAYERS])\n",
    "while True:\n",
    "    ryo, rh = sess.run([Y, H], feed_dict={\n",
    "        \"Placeholder\": ry, # X\n",
    "        \"Placeholder_4\": 1.0, # Drop out keep prob\n",
    "        \"Placeholder_2\": rh, # Hin\n",
    "        \"batch_size\": 1 # Batch_size\n",
    "    })\n",
    "    rc = sample(ryo, top=2) # Sample from the top probabilites\n",
    "    print(int_to_char([rc.tolist()]), end=\"\")\n",
    "    ry = np.array([[rc]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
